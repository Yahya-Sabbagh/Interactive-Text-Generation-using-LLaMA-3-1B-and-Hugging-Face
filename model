from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import torch
from config import device, USE_CUDA

# Model name
model_name = "meta-llama/Llama-3.2-1B"

# Load tokenizer and config
tokenizer = AutoTokenizer.from_pretrained(model_name)
config = AutoConfig.from_pretrained(model_name)

# Adjust rope scaling for longer sequences
config.rope_scaling = {'type': 'linear', 'factor': 32.0}

# Load the model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if USE_CUDA else torch.float32,
    device_map="auto" if USE_CUDA else None,
    config=config
).to(device)

print(f" Model loaded on: {device}")

# Updated chat function with conversation history
def chat(prompt, history=None, max_length=100, temperature=0.9, top_p=0.9):
    """
    prompt: current user input
    history: list of previous (user, bot) tuples
    """
    if history:
        # Concatenate all previous interactions
        conversation = ""
        for user_msg, bot_msg in history:
            conversation += f"User: {user_msg}\nBot: {bot_msg}\n"
        conversation += f"User: {prompt}\nBot:"
    else:
        conversation = f"User: {prompt}\nBot:"

    inputs = tokenizer(conversation, return_tensors="pt").to(device)
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            repetition_penalty=1.2
        )
    response = tokenizer.decode(output[0], skip_special_tokens=True)

    # Extract only the new bot response
    if "Bot:" in response:
        response = response.split("Bot:")[-1].strip()

    print("Bot response:", response)
    return response

        )
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print("Response:", response)
    return response
